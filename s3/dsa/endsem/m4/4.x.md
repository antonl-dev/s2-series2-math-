# Module 4: Sorting and Searching - Comprehensive Questions

---

### **Section 1: Sorting Algorithms (Theory and Comparison)**

1.  **Define Sorting.** Differentiate between the following pairs of sorting concepts with a clear example for each:
    *   In-place vs. Not-in-place Sorting
    *   Stable vs. Unstable Sorting
    *   Adaptive vs. Non-Adaptive Sorting

2.  **Compare Merge Sort and Quick Sort** based on the following criteria:
    *   Time Complexity (Best, Average, Worst)
    *   Space Complexity
    *   Stability
    *   Working Principle (How they implement "Divide and Conquer")

3.  **Explain the working of the Heap Sort algorithm.** Your explanation should include:
    *   The definition of a Max-Heap and the heap property.
    *   The two main phases of the algorithm (Heapify and Extraction).
    *   The purpose and complexity of the `heapify` subroutine.

4.  **Discuss any four types of sorting algorithms.** For each, briefly explain its basic idea and mention its worst-case time complexity. *(This is a general theory question, similar to those in your internal exams.)*

### **Section 2: Algorithm Application and Tracing**

5.  **You are given the array `A = {42, 24, 32, 12, 44, 33, 55}`.** Sort this array using the **Heap Sort** algorithm. Show the following:
    *   The array represented as a binary tree before sorting.
    *   The state of the array after the initial "Build Max-Heap" phase.
    *   The state of the array after the first two elements (the largest two) have been extracted and placed in their final sorted positions.

6.  **Apply Quick Sort to the array `A = {38, 27, 43, 3, 9, 82, 10}`.** Use the first element as the pivot. Show the state of the array after each partitioning step is complete.

7.  **Demonstrate the working of Merge Sort** on the array `A = {50, 25, 75, 10, 40, 90, 60}`. Clearly show the "Divide" steps (splitting the array) and the "Combine" steps (merging the subarrays).

### **Section 3: External Sorting and Searching**

8.  **Explain the concept of External Sorting.** Why is an algorithm like Quick Sort not suitable for sorting a file that is too large to fit in memory? Describe the two main phases of the External Merge Sort algorithm.

9.  **Explain the difference between Linear Search and Binary Search** in terms of their algorithm, prerequisites, and time complexity. In what specific scenario would Linear Search be a better choice than sorting the data and then using Binary Search?

10. **Trace the execution of Binary Search** to find the number **24** in the sorted array `A = {4, 8, 12, 16, 24, 29, 33, 41}`. At each step, show the values of the `low`, `high`, and `mid` pointers and the subarray being considered.

---

### **File 2 of 2: Answer Key**

# Module 4: Sorting and Searching - Detailed Answer Key

---

### **Section 1: Sorting Algorithms (Theory and Comparison)**

**1. Define Sorting & Differentiate Concepts**

**Sorting** is the process of arranging a collection of items into a specific, prescribed order, such as numerical or alphabetical order.

*   **In-place vs. Not-in-place Sorting:**
    *   **In-place:** An algorithm that sorts the data within the original array structure, requiring only a constant amount of extra memory (O(1)). Example: Bubble Sort, Heap Sort.
    *   **Not-in-place:** An algorithm that requires additional memory, typically proportional to the input size (O(n)), to hold the sorted data. Example: Merge Sort, which uses temporary arrays for merging.

*   **Stable vs. Unstable Sorting:**
    *   **Stable:** An algorithm that preserves the original relative order of equal elements. If `(5, 'a')` appears before `(5, 'b')` in the input, it will remain so in the output. Example: Merge Sort, Insertion Sort.
    *   **Unstable:** An algorithm that does not guarantee the original order of equal elements. The relative order of `(5, 'a')` and `(5, 'b')` might be swapped. Example: Quick Sort, Heap Sort.

*   **Adaptive vs. Non-Adaptive Sorting:**
    *   **Adaptive:** An algorithm whose performance improves if the input data is already partially sorted. Example: Insertion Sort runs in O(n) time on a nearly-sorted list.
    *   **Non-Adaptive:** An algorithm whose performance is independent of the initial order of elements. It performs the same operations regardless. Example: Selection Sort, Heap Sort.

---
**2. Compare Merge Sort and Quick Sort**

| Feature | Quick Sort | Merge Sort |
| :--- | :--- | :--- |
| **Time Complexity** | **Average:** O(n log n)<br>**Worst:** O(n²) | **All cases:** O(n log n) |
| **Space Complexity** | O(log n) average (recursion stack) | O(n) (temporary arrays) |
| **Stability** | No | Yes |
| **Working Principle**| Chooses a pivot and partitions the array into two parts (smaller and larger than the pivot), then sorts the parts recursively. | Divides the array into two halves, recursively sorts them, and then merges the two sorted halves back together. |

---
**3. Explain the working of Heap Sort**

Heap Sort uses a Max-Heap data structure to sort an array in-place.
*   **Max-Heap Property:** A Max-Heap is a nearly complete binary tree where the value of each parent node is greater than or equal to the values of its children. This ensures the largest element is always at the root.
*   **Phase 1: Build Max-Heap (Heapify Phase):** The algorithm first converts the input array into a Max-Heap. This is efficiently done by starting from the last non-leaf node (at index `n/2 - 1`) and calling the `heapify` function on each node up to the root. This phase places the largest element of the array at the root (index 0).
*   **Phase 2: Extraction Phase:**
    1. Swap the root element (the current maximum) with the last element of the heap.
    2. Reduce the heap size by 1. The swapped element is now in its final sorted position at the end of the array.
    3. Call `heapify` on the new root (at index 0) to restore the Max-Heap property for the reduced heap.
    4. Repeat this process until the heap size is 1.
*   **`heapify` Subroutine:** This key function takes a node and ensures it satisfies the heap property within its subtree, assuming its children's subtrees are already heaps. It compares the node with its children, swaps with the largest if needed, and recursively calls itself on the affected child. Its complexity is O(log n).

---
**4. Discuss Four Types of Sorting Algorithms**

1.  **Bubble Sort:** A simple algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. **Worst-Case:** O(n²).
2.  **Insertion Sort:** Builds the final sorted array one item at a time. It iterates through the input elements and inserts each element into its correct position in the already-sorted part of the array. **Worst-Case:** O(n²).
3.  **Merge Sort:** A Divide and Conquer algorithm. It divides the array into halves, recursively sorts them, and then merges the sorted halves. **Worst-Case:** O(n log n).
4.  **Quick Sort:** A Divide and Conquer algorithm. It picks a 'pivot' element, partitions the array around the pivot, and recursively sorts the subarrays. **Worst-Case:** O(n²).

---
### **Section 2: Algorithm Application and Tracing**

**5. Trace Heap Sort on `A = {42, 24, 32, 12, 44, 33, 55}` (n=7)**

*   **Initial Tree (not a heap):**
    ```
          42
         /  \
       24    32
      / \   /  \
     12 44 33  55 
    ```
*   **State after Build Max-Heap Phase:** The `heapify` process is applied from index `n/2 - 1 = 2` down to 0. The final heap is:
    **Array:** `[55, 44, 42, 12, 24, 33, 32]`

*   **After 1st Extraction:**
    1.  Swap root (55) with last element (32): `[32, 44, 42, 12, 24, 33, | 55]`
    2.  `heapify` the root (32) on the reduced heap (size 6).
    **Array:** `[44, 33, 42, 12, 24, 32, | 55]`

*   **After 2nd Extraction:**
    1.  Swap root (44) with last element (32): `[32, 33, 42, 12, 24, | 44, 55]`
    2.  `heapify` the root (32) on the reduced heap (size 5).
    **Array:** `[42, 33, 32, 12, 24, | 44, 55]`

---
**6. Trace Quick Sort on `A = {38, 27, 43, 3, 9, 82, 10}` (Pivot=First Element)**

1.  **`quickSort(A, 0, 6)`:**
    -   **Pivot:** 38.
    -   Partition the array. Elements `< 38` go left, `> 38` go right.
    -   **After Partition:** `[10, 27, 3, 9, 38, 82, 43]`. Pivot 38 is at index 4.
    -   Recursive calls: `quickSort(A, 0, 3)` on `[10, 27, 3, 9]` and `quickSort(A, 5, 6)` on `[82, 43]`.

2.  **`quickSort(A, 0, 3)` on `[10, 27, 3, 9]`:**
    -   **Pivot:** 10.
    -   **After Partition:** `[3, 9, 10, 27]`. Pivot 10 is at index 2.
    -   Recursive calls: `quickSort(A, 0, 1)` on `[3, 9]` and `quickSort(A, 3, 3)` on `[27]`.
    -   ...and so on until all base cases are reached.

3.  **`quickSort(A, 5, 6)` on `[82, 43]`:**
    -   **Pivot:** 82.
    -   **After Partition:** `[43, 82]`. Pivot 82 is at index 6.

Eventually, the array is fully sorted to `[3, 9, 10, 27, 38, 43, 82]`.

---
**7. Trace Merge Sort on `A = {50, 25, 75, 10, 40, 90, 60}`**

*   **Divide Phase:**
    - `[50, 25, 75, 10]` & `[40, 90, 60]`
    - `[50, 25]` & `[75, 10]` | `[40, 90]` & `[60]`
    - `[50]` `[25]` | `[75]` `[10]` | `[40]` `[90]` | `[60]`

*   **Combine Phase:**
    - Merge `[50]` & `[25]` -> `[25, 50]`
    - Merge `[75]` & `[10]` -> `[10, 75]`
    - Merge `[40]` & `[90]` -> `[40, 90]`
    - `[60]` remains `[60]`
    - Merge `[25, 50]` & `[10, 75]` -> `[10, 25, 50, 75]`
    - Merge `[40, 90]` & `[60]` -> `[40, 60, 90]`
    - **Final Merge:** Merge `[10, 25, 50, 75]` & `[40, 60, 90]` -> `[10, 25, 40, 50, 60, 75, 90]`

---
### **Section 3: External Sorting and Searching**

**8. Explain External Sorting**

**External Sorting** is used when data is too large to fit in RAM. The data resides on disk, which is much slower to access.
*   **Why Quick Sort is Unsuitable:** Quick Sort relies on random access to elements for its partitioning step (swapping distant elements). Disk access is slow and best done sequentially. The random-access nature of Quick Sort would lead to an excessive number of slow disk seek operations, making it extremely inefficient for external sorting.
*   **External Merge Sort Phases:**
    1.  **Run Creation Phase:** Read chunks of the file into RAM, sort each chunk using an internal algorithm (like Quick Sort), and write the sorted chunk (a "run") back to disk. This is repeated until the entire file is converted into multiple sorted runs.
    2.  **Merge Phase:** Perform a "K-way merge" on the runs. Read the beginning of each run into an input buffer in RAM. Repeatedly find the smallest element among all buffers, write it to an output buffer, and replenish the input buffer. When the output buffer is full, write it to the final sorted file on disk. This minimizes disk I/O by using large, sequential reads and writes.

---
**9. Difference between Linear and Binary Search**

| Feature | Linear Search | Binary Search |
| :--- | :--- | :--- |
| **Algorithm** | Checks elements one by one. | Halves the search space in each step. |
| **Prerequisites** | None. Works on any list. | List **must** be sorted. |
| **Time Complexity** | O(n) worst-case. | O(log n) worst-case. |

**When to prefer Linear Search:**
You would choose Linear Search over sorting and then using Binary Search if you only need to perform **one or very few searches**. The cost of sorting the entire list first (O(n log n)) would outweigh the benefit of a single fast search (O(log n)). The total time would be `O(n log n) + O(log n)`, whereas a single linear search is just `O(n)`.

---
**10. Trace Binary Search for key `24` in `A = {4, 8, 12, 16, 24, 29, 33, 41}`**

**Initial State:** `low = 0`, `high = 7`

1.  **Step 1:**
    -   `mid = 0 + (7 - 0) / 2 = 3`
    -   `arr[mid]` is `arr[3] = 16`.
    -   `16 < 24`, so the key must be in the right half.
    -   New range: `low = mid + 1 = 4`, `high = 7`. Subarray: `{24, 29, 33, 41}`.

2.  **Step 2:**
    -   `mid = 4 + (7 - 4) / 2 = 4 + 1 = 5`
    -   `arr[mid]` is `arr[5] = 29`.
    -   `29 > 24`, so the key must be in the left half.
    -   New range: `low = 4`, `high = mid - 1 = 4`. Subarray: `{24}`.

3.  **Step 3:**
    -   `mid = 4 + (4 - 4) / 2 = 4`
    -   `arr[mid]` is `arr[4] = 24`.
    -   `24 == 24`. The key is found.
    -   **Return index 4.**
