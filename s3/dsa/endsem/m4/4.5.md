# Module 4.5: External Memory Sorting

## 1. What is External Sorting?

**External Sorting** is a class of sorting algorithms used when the data to be sorted is too large to fit into the main memory (RAM) of a computer. The data resides primarily on secondary storage (like a hard disk drive or solid-state drive), and chunks of data are read into RAM for processing.

| Feature | Internal Sorting | External Sorting |
| :--- | :--- | :--- |
| **Data Location** | Entire dataset fits in RAM. | Dataset is on secondary storage (disk). |
| **Access** | Fast, random access to any element. | Slow, sequential access is preferred. |
| **Bottleneck** | CPU processing speed. | Disk I/O (Input/Output) operations. |
| **Goal** | Minimize comparisons and swaps. | **Minimize the number of disk reads and writes.** |
| **Examples** | Quick Sort, Heap Sort, Merge Sort (internal version). | External Merge Sort. |

The primary challenge with external sorting is that accessing data from a disk is significantly slower than accessing it from RAM. Therefore, the algorithms must be designed to minimize disk I/O by reading and writing data in large, sequential blocks.

## 2. The External Merge Sort Algorithm

The most common algorithm for external sorting is **External Merge Sort**. It adapts the standard Merge Sort's "Divide and Conquer" strategy to work efficiently with disk-based data. It operates in two distinct phases.

[**Google Image Search:** `external merge sort diagram`](https://www.google.com/search?tbm=isch&q=external+merge+sort+diagram)
*   **What to look for:** A diagram showing two phases. The first phase shows a large unsorted file being broken into smaller sorted "runs". The second phase shows these runs being merged together to produce the final sorted file.

### Phase 1: Sort (Run Creation)

In this phase, the large unsorted file is broken down into a set of smaller, sorted sub-files called **runs**.

1.  **Read a Chunk:** Read a portion of the data from the disk that can fit entirely into the available RAM. This chunk is a block of data.
2.  **Sort In-Memory:** Use an efficient internal sorting algorithm (like Quick Sort or Heap Sort) to sort this chunk completely within RAM.
3.  **Write the Run:** Write the sorted chunk back to the disk as a new temporary file. This sorted file is called a **run**.
4.  **Repeat:** Repeat steps 1-3 until all the data from the original file has been processed and written into multiple sorted runs.

At the end of this phase, the single large, unsorted file has been replaced by `N` smaller, individually sorted files (runs) on the disk.

### Phase 2: Merge (K-Way Merge)

In this phase, the `N` sorted runs are merged into a single, final sorted file.

1.  **Set Up Buffers:** Allocate memory in RAM for `N` input buffers (one for each run) and one output buffer. The available RAM is divided among these buffers.
2.  **Initial Read:** Read the first block of data from each of the `N` runs into its corresponding input buffer.
3.  **Merge Loop:**
    a. Find the smallest element among all the first elements currently in the `N` input buffers.
    b. Move this smallest element to the output buffer.
    c. Take the next element from the input buffer from which the smallest element was taken.
    d. If an input buffer becomes empty, read the next block of data from its corresponding run file on the disk.
    e. If the output buffer becomes full, write its contents to the final sorted file on disk and clear it.
4.  **Repeat:** Continue the merge loop until all input buffers have been exhausted and all runs are fully processed. The output file on the disk is now the complete sorted dataset.

## 3. Example: Sorting a 1 TB Dataset

Let's illustrate the process with the example from your exam paper.

**Problem:** Sort a 1 TB dataset using external memory techniques, assuming you have **1 GB** of available RAM for sorting.

**Dataset Size:** 1 TB = 1024 GB
**Available RAM:** 1 GB

#### **Phase 1: Run Creation**

1.  **Read 1 GB** of data from the 1 TB file into RAM.
2.  **Sort** this 1 GB chunk in memory using Quick Sort.
3.  **Write** the 1 GB sorted run to a temporary file on the disk.
4.  **Repeat** this process for the entire 1 TB file.

-   **Number of Runs:** Total Size / RAM Size = 1024 GB / 1 GB = **1024 runs**.
-   **Result of Phase 1:** You now have 1024 temporary files on disk, each 1 GB in size and each internally sorted.

#### **Phase 2: K-Way Merge (with K = 1024)**

1.  **Allocate Buffers:** We need to merge 1024 runs. We divide our 1 GB of RAM for this purpose.
    -   Let's allocate 100 MB for the **output buffer**.
    -   This leaves 900 MB for the **1024 input buffers**.
    -   Size of each input buffer = 900 MB / 1024 â‰ˆ 0.88 MB.

2.  **Merge Process:**
    a. Read the first 0.88 MB from **each** of the 1024 sorted run files into its respective input buffer in RAM.
    b. Perform a 1024-way merge. Repeatedly find the smallest element among the current records in all 1024 input buffers (a min-heap is often used for this in practice to do this efficiently).
    c. Place the smallest element into the 100 MB output buffer.
    d. When an input buffer runs out of data, refill it by reading the next 0.88 MB block from its corresponding 1 GB run file on disk.
    e. When the 100 MB output buffer is full, write its entire contents to the final, single 1 TB sorted output file on disk. Then, empty the output buffer to continue filling it.
    f. This process continues until all 1024 runs have been fully read and merged.

At the end of this phase, you will have a single 1 TB file on disk containing all the data in sorted order.

---

### Questions

<details>
  <summary><b>1. You are tasked with sorting a dataset that cannot fit into main memory. Explain how the External Merge Sort algorithm works and show how you would sort a dataset of size 1 TB using 1 GB of RAM. Illustrate the process with intermediate steps.</b></summary>
  
  **Answer:**

  External Merge Sort is an algorithm designed for sorting massive datasets that reside on disk. It works by minimizing slow disk I/O operations. The algorithm is divided into two main phases:

  **Phase 1: Run Creation (Sort Phase)**
  1. The algorithm reads a chunk of data from the large file that can fit into the available RAM (in this case, 1 GB).
  2. It sorts this chunk in-memory using an efficient internal sorting algorithm like Quick Sort.
  3. The sorted chunk, now called a "run," is written to a new temporary file on the disk.
  4. This process is repeated until the entire 1 TB file is converted into multiple sorted runs.

  *Intermediate Step for 1 TB dataset:*
  - Number of runs = (Total Data Size) / (RAM Size) = 1024 GB / 1 GB = **1024 runs**.
  - The result of this phase is 1024 separate files on disk, each 1 GB in size and internally sorted.

  **Phase 2: Merge Phase (K-Way Merge)**
  1. The 1024 sorted runs are now merged into a single output file.
  2. Small portions of each run are read into input buffers in RAM. An output buffer is also created.
  3. The algorithm repeatedly finds the smallest element among all the input buffers, moves it to the output buffer, and replaces it with the next element from the same run.
  4. When an input buffer is empty, it's refilled from its corresponding run file on disk. When the output buffer is full, its content is written to the final sorted file on disk.

  *Intermediate Step for 1 TB dataset:*
  - We perform a 1024-way merge.
  - The 1 GB RAM is divided into 1024 small input buffers and one large output buffer.
  - The algorithm reads a small block from each of the 1024 files, merges them, and writes the sorted data sequentially to the final 1 TB output file. This ensures that disk I/O is mostly sequential, which is much faster than random access.
</details>

<details>
  <summary><b>2. Differentiate between internal and external sorting. What is the main bottleneck that external sorting algorithms are designed to overcome?</b></summary>
  
  **Internal Sorting** refers to sorting algorithms that operate on data that can fit entirely within the main memory (RAM). The main bottleneck for these algorithms is CPU performance (number of comparisons and swaps). Examples include standard Quick Sort, Heap Sort, and Insertion Sort.

  **External Sorting**, on the other hand, is used for datasets that are too large for RAM and must be stored on secondary storage (like a disk).

  The main bottleneck that external sorting algorithms are designed to overcome is **disk I/O (Input/Output)**. Accessing data from a disk is thousands of times slower than accessing it from RAM. External sorting algorithms, like External Merge Sort, are engineered to minimize the number of times data must be read from and written to the disk. They achieve this by processing data in large, sequential chunks rather than accessing individual elements randomly.
</details>

<details>
  <summary><b>3. Why is the "merge" phase in External Merge Sort necessary? Why can't we just sort each run and then concatenate them?</b></summary>
  
  The "merge" phase is absolutely necessary because simply sorting individual runs and concatenating them does not result in a globally sorted file.

  For example, if we sort a list `[8, 6, 2, 7, 3, 5, 4, 1]` with enough memory for 4 elements at a time:

  - **Phase 1 (Run Creation):**
    - **Run 1:** Sort `[8, 6, 2, 7]` -> `[2, 6, 7, 8]`
    - **Run 2:** Sort `[3, 5, 4, 1]` -> `[1, 3, 4, 5]`

  - **Concatenation (Incorrect Approach):**
    - If we just concatenate these runs, we get `[2, 6, 7, 8, 1, 3, 4, 5]`. This file is clearly not sorted.

  - **Merge Phase (Correct Approach):**
    - The merge phase intelligently combines the two sorted runs `[2, 6, 7, 8]` and `[1, 3, 4, 5]` by comparing their elements one by one, ensuring the final output is `[1, 2, 3, 4, 5, 6, 7, 8]`. The merge step is what establishes the correct global order across all the individually sorted runs.
</details>
