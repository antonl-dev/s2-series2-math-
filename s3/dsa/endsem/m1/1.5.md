# Module 1: Basic Concepts of Data Structures

## 1.4 Analyzing Programs and Space Bounds

Now that we understand the theoretical concepts of complexity, we need to apply them to actual code. This section covers the practical rules for analyzing the time complexity of programs and introduces the concept of Space Complexity.

### 1. Analyzing Programs: Rules for Time Complexity

When analyzing code, we count the number of primitive operations. Here are the general rules:

**Rule 1: Simple Statements (O(1))**
A simple statement like an assignment, a read/write operation, or an arithmetic operation takes a constant amount of time. We denote this as **O(1)**.
```c
x = x + 1; // O(1)
int a = 5; // O(1)
```

**Rule 2: Sequential Statements (Sum of complexities)**
If you have a sequence of statements, the total time is the sum of their individual times. The dominant term determines the final complexity.
```c
// O(1) + O(n) + O(n^2) = O(n^2)
statement1; // O(1)
for (int i = 0; i < n; i++) { ... } // O(n)
for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) { ... } // O(n^2)
}
```

**Rule 3: Loops (Number of iterations × Complexity of loop body)**
The time complexity of a loop is the number of times it iterates multiplied by the complexity of the statements inside the loop.

*   **Linear Loop:** A loop that increments by a constant value.
    ```c
    // Loop runs n times. Body is O(1). Total = n * O(1) = O(n)
    for (int i = 0; i < n; i++) {
        x = x + 1;
    }
    ```
*   **Nested Loops:** Multiply the complexities of the inner and outer loops.
    ```c
    // Outer loop runs n times. Inner loop runs n times. Body is O(1).
    // Total = n * (n * O(1)) = O(n^2)
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < n; j++) {
            x = x + 1;
        }
    }
    ```
*   **Logarithmic Loop:** A loop where the control variable is multiplied or divided by a constant. This is a key example from your question paper.
    ```c
    // This loop runs log(n) times.
    for (int j = 1; j < n; j = j * 2) {
        x = x + 1;
    }
    ```
    **Explanation:** The value of `j` goes `1, 2, 4, 8, 16, ...`. If the loop runs `k` times, `j = 2^k`. The loop stops when `2^k ≥ n`. Taking the logarithm on both sides gives `k ≥ log₂(n)`. Therefore, the complexity is **O(log n)**.

**Rule 4: Conditional Statements (if-else)**
The complexity is the time for the condition check (usually O(1)) plus the complexity of the more expensive branch (worst-case).
```c
if (condition) {
    // block 1 with complexity O(n)
} else {
    // block 2 with complexity O(n^2)
}
// The total complexity of this block is O(n^2)
```

### 2. Space Bounds (Space Complexity)

**Space Complexity** is the amount of memory an algorithm needs to run to completion. It is composed of two parts:

1.  **Instruction Space:** The memory used to store the compiled version of the code. This is typically fixed and constant for a given program, so it is often ignored in asymptotic analysis.

2.  **Data Space:** The memory used to store all variables, constants, and data structures. This is the part we focus on, and it is further divided into:
    *   **Fixed Part:** Memory that is independent of the input size (e.g., simple variables, constants).
    *   **Variable Part:** Memory that depends on the input size. This includes:
        *   Dynamically allocated memory (e.g., using `malloc`).
        *   The recursion stack space used by function calls.

The space complexity `S(P)` of a program `P` is `S(P) = c + S_p(I)`, where `c` is the fixed part and `S_p(I)` is the variable part that depends on the input instance `I`.

[Click here for a diagram showing Recursion Stack Space](https://www.google.com/search?tbm=isch&q=recursion+stack+space+complexity+diagram)
**What to look for:** A visual representation of a "call stack" where each recursive function call adds a new "stack frame" on top, consuming more memory.

#### Example: Iterative vs. Recursive Sum

Consider an algorithm to find the sum of elements in an array.

**1. Iterative Version (In-place)**
```c
int sum(int arr[], int n) {
    int total = 0; // O(1) space for 'total'
    for (int i = 0; i < n; i++) { // O(1) space for 'i'
        total += arr[i];
    }
    return total;
}
```
*   **Space Analysis:** The memory required is for the array pointer `arr`, the size `n`, the variable `total`, and the loop counter `i`. None of these depend on the size of the array `n` itself (the array's space is considered input space). The **auxiliary space** (extra space used) is constant.
*   **Space Complexity: O(1)**

**2. Recursive Version**
```c
int sum_recursive(int arr[], int n) {
    if (n <= 0) {
        return 0;
    }
    return arr[n-1] + sum_recursive(arr, n-1);
}
```
*   **Space Analysis:** Each time the function calls itself, a new **stack frame** is created in memory to store its local variables and the return address. For an array of size `n`, the function will call itself `n` times before reaching the base case. This creates a chain of `n` stack frames.
*   **Space Complexity: O(n)** because the depth of the recursion stack is proportional to the input size `n`.

---

### Questions

1.  What is space complexity? What are the different components that make up the space used by a program?
2.  Analyze the time complexity of the following code snippet. Explain your reasoning.
    ```c
    for (int i = 0; i < n; i++) {
        for (int j = 1; j < n; j = j * 2) {
            // Some O(1) operation
        }
    }
    ```
3.  Compare the space complexity of an iterative algorithm versus its equivalent recursive algorithm. Use an example.
4.  Why do we usually ignore constant factors and lower-order terms when analyzing complexity?

<details>
<summary>Click to see Answers</summary>

---

#### Answer to Question 1

**Space complexity** is the total amount of memory space required by an algorithm or program to run to completion. It is a measure of the algorithm's memory efficiency.

The space used by a program is made up of two main components:
1.  **Instruction Space:** This is the memory required to store the executable code of the program. It is generally a fixed size for a given program and does not change with the input size.
2.  **Data Space:** This is the memory required to store all variables, constants, and data structures. It is further divided into:
    *   **Fixed Part:** Space for variables whose size is independent of the input size (e.g., simple variables like `int i;`).
    *   **Variable Part:** Space whose size depends on the input instance. This is the most critical part for analysis and includes dynamically allocated memory (like `malloc`) and the space used by the recursion stack.

---

#### Answer to Question 2

The given code snippet is:
```c
for (int i = 0; i < n; i++) {
    for (int j = 1; j < n; j = j * 2) {
        // Some O(1) operation
    }
}
```
**Analysis:**

1.  **Outer Loop:** The outer loop `for (int i = 0; i < n; i++)` is a standard linear loop. It runs exactly `n` times.
2.  **Inner Loop:** The inner loop `for (int j = 1; j < n; j = j * 2)` is a logarithmic loop. The control variable `j` doubles in each iteration (1, 2, 4, 8, ...). This loop will execute approximately `log₂(n)` times before `j` becomes greater than or equal to `n`. Therefore, the complexity of the inner loop is **O(log n)**.
3.  **Total Complexity:** Since the loops are nested, we multiply their complexities. The total time complexity is the number of iterations of the outer loop multiplied by the number of iterations of the inner loop.

    `Total Complexity = n * O(log n) = O(n log n)`

The time complexity of the code is **O(n log n)**.

---

#### Answer to Question 3

The space complexity of an iterative algorithm is often different from its recursive counterpart, primarily due to the **recursion stack**.

*   **Iterative Algorithm:** Usually uses a fixed number of variables (loop counters, accumulators). As long as it doesn't use dynamic data structures that grow with the input, its auxiliary space complexity is typically **O(1)** (constant).
*   **Recursive Algorithm:** Each recursive call adds a new frame to the system's call stack to store local variables, parameters, and the return address. The space required is proportional to the maximum depth of the recursion.

**Example: Calculating Factorial**

*   **Iterative Factorial (Space: O(1))**
    ```c
    int factorial_iter(int n) {
        int result = 1;
        for (int i = 1; i <= n; i++) {
            result *= i;
        }
        return result;
    }
    // Uses a few fixed variables (result, i). Space is O(1).
    ```
*   **Recursive Factorial (Space: O(n))**
    ```c
    int factorial_recur(int n) {
        if (n == 0) return 1;
        return n * factorial_recur(n - 1);
    }
    // To calculate factorial(n), it makes n recursive calls.
    // This creates n frames on the call stack. Space is O(n).
    ```
In this case, the iterative solution is more space-efficient.

---

#### Answer to Question 4

We ignore constant factors and lower-order terms in asymptotic analysis because we are interested in the **rate of growth** of the algorithm's runtime as the input size `n` becomes very large.

1.  **Constant Factors are Machine-Dependent:** A constant factor (e.g., the `5` in `5n²`) depends on the specific hardware, compiler, and programming language. A faster computer might reduce this constant, but the overall growth rate remains the same. Asymptotic analysis aims to be machine-independent.

2.  **Lower-Order Terms Become Insignificant:** For a large input `n`, the highest-order term dominates the function's value. For example, in `T(n) = n² + 100n`, if `n = 1,000,000`:
    *   `n² = 1,000,000,000,000`
    *   `100n = 100,000,000`
    The `n²` term is vastly larger and contributes almost all of the final value. The `100n` term becomes negligible in comparison. By focusing on the dominant term (`n²`), we get a simple and accurate picture of how the algorithm scales.

</details>
